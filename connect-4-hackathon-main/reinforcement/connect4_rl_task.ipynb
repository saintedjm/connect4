{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8763b689",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "\n",
    "Starting with this notebook you might either gain your first experience in Reinforcement Learning (RL) or refresh existing knowledge.\n",
    "\n",
    "The goal is to familiarize yourself with the concept of environments (in our case `Connect 4`) and to use the relevant libraries required for this endeavor.\n",
    "\n",
    "Throughout this notebook, you might stumble upon various errors, bugs and mistakes. Some of which might even be novel to us ;). But those should be considered as part of your learning journey, not as a hindrance.\n",
    "\n",
    "In the end, you ideally have some running code, the respective AI player and an intuition of how RL could be used in the context of games. You might leave with more open questions than you might have entered, but this could be the prolog of your learning adventure :). To clarify, using RL for Connect 4 without further do will not produce optimal results. It will not even parallel the Minimax algorithm. Try not to use LLMs like ChatGPT unless you are really stuck at some point. We have a reference solution to the problem and provide a skeleton of what should be implemented beneath. Have fun and hopefully, we'll be working on some projects together ;).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from rl_task import DQN, preprocess_obs\n",
    "from setup import setup\n",
    "from gym_env import Connect4Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup()\n",
    "env = Connect4Env(render_mode=\"human\")\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1615417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment and play a few rounds!\n",
    "\n",
    "from play_against_random import play_against_random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# play_against_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13a806",
   "metadata": {},
   "source": [
    "Familiarize yourself with the following functions of the environment and roughly about how the environment works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982432b",
   "metadata": {},
   "source": [
    "Feel free to rerun the cell below and look at what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154f47b",
   "metadata": {},
   "source": [
    "Intermediate questions: How does an environment work (at least the most important functions: step() and reset())? How might render() work with the arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5661cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    action = np.random.choice([0,1,2,3,4,5,6])\n",
    "    print(\"\\nAfter action\", action, \"\\n\")\n",
    "    env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0699dc",
   "metadata": {},
   "source": [
    "In the following, we set up some hyperparameters. We will stumble upon those later on. Maybe you already know a few of these. If you want to know what they do or what hyperparameters in general are: Please, do not hesitate and ask ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "GAMMA = 0.99\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "MEM_SIZE = 10000\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.99\n",
    "TARGET_UPDATE = 10\n",
    "EVAL_INTERVAL = 20\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1739fa87",
   "metadata": {},
   "source": [
    "In the following, we will implement a class `ReplayBuffer`, it is supposed to store past episodes. We will randomly sample batches from those when training to prevent overfitting on recent transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11211d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        ''' \n",
    "        Initialize the buffer with a capacity of maxlen\n",
    "        '''\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        '''\n",
    "        Push data into buffer\n",
    "        '''\n",
    "        self.buffer.append(args)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        randomly sample batch_size transitions from the buffer.\n",
    "        return numpy arrays of each where it proves reasonable.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to NumPy arrays (optional: convert to tensors later)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        return the length of the buffer\n",
    "        '''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707394f",
   "metadata": {},
   "source": [
    "Next, implement the agent class `DQNAgent`. This is our agent that interacts with the Connect 4 environment. Before continuing, please, continue with the `rl_task.py` file.\n",
    "\n",
    "There, you will implement a first Neural Network that approximates the q-function in Connect 4. It will be the \"brain\" of our agent. The class `DQNAgent` will provide its \"body\" to perform valid actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc87e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, obs_shape, n_actions):\n",
    "        # Set the number of actions\n",
    "        self.n_actions = n_actions\n",
    "        self.device = DEVICE\n",
    "        \n",
    "        # Initalize a \"Policy Net\", it should be the Neural Network that is regularly updated by backpropagation\n",
    "        self.policy_net = DQN(obs_shape, n_actions).to(self.device)\n",
    "        \n",
    "        # Initalize a target net that is a more stable Neural Network, i.e., its weights are copied from the \n",
    "        # Policy net every, e.g., 10th or 20th iteration \n",
    "        self.target_net = DQN(obs_shape, n_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Use the Adam Optimizer, Buffer and epsilon for Epsilon Greedy Exploration\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LR)\n",
    "\n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(capacity=MEM_SIZE)\n",
    "\n",
    "        # Epsilon-greedy parameters\n",
    "        self.epsilon = EPS_START\n",
    "\n",
    "        return\n",
    "\n",
    "    def select_action(self, state, legal_actions, use_epsilon=True):\n",
    "        # Epsilon greedy decision\n",
    "        if use_epsilon and random.random() < self.epsilon:\n",
    "            # Explore: Choose random legal action\n",
    "            return random.choice(legal_actions)\n",
    "            \n",
    "        # compute the q_values\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(DEVICE)  # shape: (1, C, H, W)\n",
    "            q_values = self.policy_net(state_tensor).squeeze(0).cpu().numpy()\n",
    "\n",
    "        # Mask illegal actions such that none of those are taken; Explicitly prohibit them for safety's sake\n",
    "        # Implement masking to prevent any illegal actions from happening\n",
    "        masked_q_values = np.full_like(q_values, -np.inf)\n",
    "        masked_q_values[legal_actions] = q_values[legal_actions]\n",
    "\n",
    "        # Choose the best action\n",
    "        action = int(np.argmax(masked_q_values))\n",
    "        \n",
    "        # In case an illegal action has been taken; Exception\n",
    "        if action not in legal_actions:\n",
    "            raise ValueError(f\"Selected illegal action: {action}\")\n",
    "\n",
    "        # Returns the selected action\n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        We calculate the target values. Those can be calculated from our target_net (the stable network). \n",
    "        To update the values, we calculate the bellman optimality equation with the target_net, to update our policy_net via backpropagation. \n",
    "        After some iterations we then also overwrite our target_net. (By backpropating the difference between target (via BOE) and the target)\n",
    "        \"\"\"\n",
    "        # Ensure that the batch size does not exceed the memory\n",
    "        if len(self.replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "        # retrieve samples from memory\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Make actions into shape [batch_size, 1] so we can use it to gather\n",
    "        actions = torch.tensor(actions, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        \n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Get predicted Q-values for all actions in the batch of states\n",
    "        q_values = self.policy_net(states)\n",
    "\n",
    "        # Gather the Q-values corresponding to the actions that were actually taken\n",
    "        state_action_values = q_values.gather(1, actions).squeeze(1)  # shape: [batch]\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Retrieve next q values and mask illegal actions for next state\n",
    "            next_q_values = self.target_net(next_states)\n",
    "            \n",
    "            max_next_q_values = next_q_values.max(1)[0]  # max over actions\n",
    "\n",
    "        # Bellman Optimality Equation Q(s, a) ← r + γ * max_a' Q(next_s, a')\n",
    "        # set target to rewards for done states (this is needed to avoid NaN values that occur when multiplying by -inf\n",
    "            \n",
    "            targets = rewards + GAMMA * max_next_q_values * (1 - dones)  # no future reward if done\n",
    "\n",
    "        # Compute loss and backpropagate it\n",
    "        loss = F.mse_loss(state_action_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        # Copy weights from policy_net to target_net\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe98397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def moving_average(data, window_size):\n",
    "    \"\"\"Compute moving average using a simple window.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12accbce",
   "metadata": {},
   "source": [
    "The following method should be used to evaluate your agent's performance. Try to figure out how to fill the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e91097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent: DQNAgent, env_name=\"Connect4-v0\", num_games=10):\n",
    "    env = gym.make(env_name, render_mode=None)\n",
    "    wins = 0\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        (obs, _), info = env.reset()\n",
    "        done = False\n",
    "        player = 0  # Agent starts\n",
    "\n",
    "        while not done:\n",
    "            legal_actions = info['legal_actions']\n",
    "\n",
    "            if player == 0:\n",
    "                # Agent's turn\n",
    "                action = agent.select_action(obs, legal_actions)\n",
    "            else:\n",
    "                # Random opponent\n",
    "                action = np.random.choice(legal_actions)\n",
    "\n",
    "            (next_obs, _), reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            obs = next_obs\n",
    "            player = 1 - player\n",
    "\n",
    "        # Agent is player 0. If agent played last and reward == 1, it won.\n",
    "        if player == 1 and reward[0] == 1:\n",
    "            wins += 1\n",
    "\n",
    "    env.close()\n",
    "    return wins / num_games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a74b23",
   "metadata": {},
   "source": [
    "Now, the hopefully final puzzle piece: Train your first Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd932d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(num_episodes: int = 1500):\n",
    "    \"\"\"\n",
    "    Train the DQNAgent by having it play against a random opponent.\n",
    "    \"\"\"\n",
    "    env = Connect4Env()              \n",
    "    obs_shape = env.observation_space[0].shape\n",
    "    n_actions = env.action_space.n\n",
    "    agent0 = DQNAgent(obs_shape, n_actions)\n",
    "    agent1 = DQNAgent(obs_shape, n_actions)\n",
    "\n",
    "    rewards_history = []\n",
    "    # Keep track of your win rates from your evaluation for the plot in the end\n",
    "    winrates = []\n",
    "\n",
    "    # baseline performance before any training\n",
    "    obs, info = env.reset()\n",
    "    state0 = preprocess_obs(obs[0])\n",
    "    winrate = evaluate_agent(agent0)\n",
    "    print(f\"Evaluation after 0 episodes: winrate vs random = {winrate:.2f}\")\n",
    "    winrates.append(winrate)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()                           # reset returns (obs_tuple, info)\n",
    "        state0 = preprocess_obs(obs[0])\n",
    "        state1 = preprocess_obs(obs[1])\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            if info['current_player'] == 0:\n",
    "                # agent’s turn\n",
    "                legal_actions = info['legal_actions']\n",
    "                action = agent0.select_action(state0, legal_actions, use_epsilon=True)\n",
    "            else:\n",
    "                # opponent's turn\n",
    "                legal_actions = info['legal_actions']\n",
    "                action = agent1.select_action(state1, legal_actions, use_epsilon=True)\n",
    "\n",
    "            # apply move\n",
    "            (next_obs, reward, terminated, truncated, info) = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # if it was our move, record reward; else ignore or subtract as needed\n",
    "            # if info['current_player'] == 1:\n",
    "            total_reward += reward[0]\n",
    "\n",
    "            if done:\n",
    "                # get the player who lost\n",
    "                if reward[0] == -1:\n",
    "                    last_elem = agent0.replay_buffer.buffer[-1]\n",
    "                    last_state, last_action, _, last_n_state, last_done = last_elem\n",
    "                    agent0.replay_buffer.buffer[-1] = (last_state, last_action, -1, last_n_state, True)\n",
    "                if reward[1] == -1:\n",
    "                    last_elem = agent1.replay_buffer.buffer[-1]\n",
    "                    last_state, last_action, _, last_n_state, last_done = last_elem\n",
    "                    agent1.replay_buffer.buffer[-1] = (last_state, last_action, -1, last_n_state, True)\n",
    "                \n",
    "            next_state0 = preprocess_obs(next_obs[0])      # player-0’s view\n",
    "            if info['current_player'] == 1:  # player 0 just played\n",
    "                agent0.replay_buffer.push(state0, action, reward[0], next_state0, done)\n",
    "                if len(agent0.replay_buffer) >= BATCH_SIZE:\n",
    "                    agent0.optimize()\n",
    "                    \n",
    "            next_state1 = preprocess_obs(next_obs[1])      # player-1’s view\n",
    "            if info['current_player'] == 0:  # player 1 just played\n",
    "                agent1.replay_buffer.push(state1, action, reward[1], next_state1, done)\n",
    "                if len(agent1.replay_buffer) >= BATCH_SIZE:\n",
    "                    agent1.optimize()\n",
    "\n",
    "            state0 = next_state0\n",
    "            state1 = next_state1\n",
    "\n",
    "        # after episode: decay exploration rate\n",
    "        agent0.epsilon = max(EPS_END, agent0.epsilon * EPS_DECAY)\n",
    "        agent1.epsilon = max(EPS_END, agent1.epsilon * EPS_DECAY)\n",
    "\n",
    "        # sync target network periodically\n",
    "        if (episode + 1) % TARGET_UPDATE == 0:\n",
    "            agent0.update_target()\n",
    "            agent1.update_target()\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(reward)\n",
    "            print(f\"Episode {episode + 1}, reward: {total_reward:.2f}, eps: {agent0.epsilon:.2f}\")\n",
    "\n",
    "        if (episode + 1) % EVAL_INTERVAL == 0:\n",
    "            winrate = evaluate_agent(agent0)\n",
    "            winrates.append(winrate)\n",
    "            print(f\"Evaluation after {episode + 1} episodes: winrate vs random = {winrate:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent0, rewards_history, winrates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc6e45",
   "metadata": {},
   "source": [
    "Let's see its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup()\n",
    "#! You might want to adapt the episodes for your need.\n",
    "agent, rewards, winrates = train_dqn(1000)\n",
    "# Save the trained policy network\n",
    "torch.save(agent.policy_net.state_dict(), \"dqn_policy_net.pth\")\n",
    "window = 10\n",
    "if len(rewards) >= window:\n",
    "    rewards_ma = moving_average(rewards, window)\n",
    "    plt.plot(range(window-1, len(rewards)), rewards_ma, label=f\"{window}-episode moving avg\")\n",
    "plt.plot(rewards, alpha=0.3, label=\"Episode reward\")\n",
    "# Plot winrate\n",
    "if winrates:\n",
    "    plt.plot(np.arange(0, len(winrates)*EVAL_INTERVAL, EVAL_INTERVAL), winrates, label=\"Winrate vs random\", color='red')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward / Winrate\")\n",
    "plt.title(\"DQN Training Rewards and Winrate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#! need to restart the kernel and rerun the entire notebook when applying changes to the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934335c",
   "metadata": {},
   "source": [
    "Now, run `play_against_rl` and see if you can defeat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29711edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from play_against_rl import play_against_agent\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "play_against_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662999c2-7173-4c7f-abf0-6f1060fc4354",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
